{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### ğŸŒ Guide to Setting Up a Google Cloud Platform (GCP) Account\n",
    "\n",
    "#### Step 1: Go to the Google Cloud Platform Website ğŸŒ\n",
    "1. Visit the [Google Cloud Platform website](https://cloud.google.com).\n",
    "2. Sign in with your Google account. If you donâ€™t have one, select **\"Create account\"** and follow the steps to set it up.\n",
    "3. Click **\"Get Started for Free\"** at the top of the page.\n",
    "4. Select your email ID and country.\n",
    "5. Click **Agree & continue** to proceed.\n",
    "\n",
    "#### Step 2: Add Payment Information (First-time Users) ğŸ’³\n",
    "1. Choose your payment profile type: **Individual** or **Organization**.\n",
    "2. Add a payment method (credit/debit card or netbanking).\n",
    "3. Press **Enter** to access the Google Cloud Console.\n",
    "\n",
    "#### Step 3: Go to the Billing Section ğŸ’°\n",
    "1. Youâ€™ll be prompted to start a **$300 Free Trial** (valid for 90 days) after signing in.\n",
    "2. Either click on **Billing** in the console menu or navigate to **Billing > Payment overview**.\n",
    "3. Complete the payment setup (initial charges may apply, such as â‚¹2 for credit cards or â‚¹1000 for netbanking).\n",
    "4. Supported cards: Mastercard/Visa with online e-commerce services enabled in your bank account.\n",
    "\n",
    "#### Step 4: Set Up Your Project ğŸ“\n",
    "1. After activation, youâ€™ll be redirected to the GCP Console.\n",
    "2. Create a new project by clicking **\"Select a project\"** in the top bar near the Google Cloud logo, then choose **\"New Project\"**.\n",
    "3. Enter your **Project name**, **Location**, and **Organization** (if applicable).\n",
    "4. Click **Create** to set up your new project.\n",
    "\n",
    "ğŸ‰ Youâ€™re now ready to start exploring GCPâ€™s features!\n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### ğŸŒ©ï¸ Guide to Setting Up Google Cloud Storage (GCS)\n",
    "\n",
    "##### Step 1: Create a Bucket in GCS\n",
    "\n",
    "1. ğŸ§­ **Navigate to Cloud Storage**: \n",
    "   - Go to the navigation menu in the top left corner.\n",
    "   - Find **Cloud Storage** (use the search bar at the top if needed).\n",
    "   - Select **Buckets**.\n",
    "\n",
    "2. â• **Create a Bucket**:\n",
    "   - Click **Create Bucket**.\n",
    "\n",
    "3. ğŸ“ **Name Your Bucket**:\n",
    "   - Pick a name for your bucket.\n",
    "   - (Optional) Check the box in the **Optimize storage** dropdown for hierarchical structures (we did not select this).\n",
    "   \n",
    "   ![Bucket Name](images/img2.png)\n",
    "\n",
    "4. ğŸŒ **Choose Data Location**:\n",
    "   - Select **Multiregion** or another option based on your preference (we used **Asia multiregion**).\n",
    "   - (Optional) Enable cross-bucket replication within the multiregion by selecting the checkbox.\n",
    "\n",
    "   ![Data Location](images/img3.png)\n",
    "\n",
    "5. ğŸ—‚ï¸ **Select Storage Class**:\n",
    "   - Choose a storage class for your data (we selected **Standard**).\n",
    "\n",
    "   ![Storage Class](images/img4.png)\n",
    "\n",
    "6. ğŸ” **Access Control**:\n",
    "   - If you want the bucket to be private, check **Enforce public access**.\n",
    "   - We chose **Uniform** for access control.\n",
    "\n",
    "   ![Access Control](images/img5.png)\n",
    "\n",
    "7. ğŸ•’ **Retention Policy**:\n",
    "   - (Optional) Enable a retention policy if you want to allow soft deletes, and set a duration for deleted files.\n",
    "   \n",
    "   ![Retention Policy](images/img6.png)\n",
    "\n",
    "8. ğŸš€ **Create the Bucket**:\n",
    "   - Click **Create**â€”your bucket is ready! ğŸ¥³\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Guide to Enable Required APIs and Basic Setup\n",
    "\n",
    "ğŸ’¡ **Note**: Use the console search bar to locate each API easily.\n",
    "\n",
    "1. ğŸ”Œ **Enable Necessary APIs**:\n",
    "   - Enable the following APIs:\n",
    "     - **IAM API**\n",
    "     - **Compute Engine API**\n",
    "     - **Vertex AI API**\n",
    "     - **Artifact Registry API**\n",
    "     - **Cloud Storage API**\n",
    "     - **Notebooks API**\n",
    "\n",
    "2. ğŸ› ï¸ **Request GPU Configurations**:\n",
    "   - Go to **IAM & Admin > Quota and Limits** to request the required GPU configurations.\n",
    "\n",
    "3. ğŸ›ï¸ **Select GPU and Set Worker Count**:\n",
    "   - Choose your preferred GPU type and specify the number of workers needed for GPU processing.\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Guide to Create a Service Account and Assign IAM Permissions\n",
    "\n",
    "### Step 1: ğŸ« Create a Service Account\n",
    "\n",
    "1. Go to **IAM & Admin > Service Account**.\n",
    "2. Click on **Create Service Account**.\n",
    "3. Enter the account details and click **Create and Continue**.\n",
    "4. Grant the necessary permissions to the service account.\n",
    "5. Click **Create**. Your service account is now ready to use! ğŸ¥³\n",
    "6. This account can now be used for new projects or within existing projects.\n",
    "\n",
    "### Step 2: ğŸ” Assign Required Permissions\n",
    "\n",
    "1. In the **IAM** panel, grant the following permissions to the service account:\n",
    "   - **AI Platform Admin**\n",
    "   - **Compute Admin**\n",
    "   - **Storage Admin**\n",
    "   - **Storage Object Admin**\n",
    "   - **Vertex AI Service Admin**\n",
    "   - **Artifact Registry Admin**\n",
    "\n",
    "2. Run the code in the section below then again apply the permissions given above.\n",
    "   - [Jump to Code Example](#service-account-detail)\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Step 1: Docker Image Requirement\n",
    "\n",
    "1. **Python 3.8 Compatibility**: \n",
    "   - The Kubeflow Pipeline (KFP) requires Python 3.8, while the default base image uses Python 3.7. \n",
    "   - To address this, weâ€™ll create a custom Docker image with Python 3.8.\n",
    "\n",
    "### Step 2: Creating the Docker Image\n",
    "\n",
    "1. **Set Up Docker**:\n",
    "   - Install Docker on your PC or use the **gcloud CLI** within **Cloud Shell** (recommended as it has Docker pre-installed).\n",
    "   - If using Docker on your PC, create a DockerHub account, and use DockerEngine.\n",
    "\n",
    "2. **Access gcloud CLI**:\n",
    "   - Open **Cloud Shell** using the icon near your account information.\n",
    "\n",
    "3. **Prepare the Environment**:\n",
    "   - Set your GCP project ID: `PROJECT_ID=\"your-gcp-project-id\"`\n",
    "   - Create a `requirements.txt` with the following content (use the `nano` command to create the file):\n",
    "\n",
    "     ![Dockerfile](images/img14.jpeg)\n",
    "\n",
    "4. **Define Dependencies**:\n",
    "   - Create a `Dockerfile` file as shown below:\n",
    "\n",
    "     ![requirements.txt](images/img15.jpeg)\n",
    "\n",
    "5. **Build and Push the Docker Image**:\n",
    "\n",
    "   #### Commands\n",
    "   - **Defining the ProjectID**\n",
    "      ```bash\n",
    "      PROJECT_ID=\"your-gcp-project-id\"\n",
    "      ```\n",
    "   - **Build the Docker image**:\n",
    "     ```bash\n",
    "     docker build -t gcr.io/$PROJECT_ID/PROJECT_NAME:latest .\n",
    "     ```\n",
    "\n",
    "   - **Push the image to Google Container Registry (GCR)**:\n",
    "     ```bash\n",
    "     docker push gcr.io/$PROJECT_ID/PROJECT_NAME:latest\n",
    "     ```\n",
    "\n",
    "6. **Ready to Use**:\n",
    "   - Your Docker image is now ready for use in the pipeline code ğŸ‰\n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Explanation of Each Library\n",
    "\n",
    "- **google-cloud-aiplatform** ğŸ§ \n",
    "  - Provides tools to create, manage, and deploy machine learning models on Google Cloud's AI Platform.\n",
    "  - Facilitates integration with Vertex AI for tasks like training, evaluation, and deployment.\n",
    "\n",
    "- **google-cloud-storage** â˜ï¸\n",
    "  - Enables interaction with Google Cloud Storage, allowing uploads and downloads of datasets, models, or other files.\n",
    "  - Ideal for managing large datasets or model artifacts in the cloud.\n",
    "\n",
    "- **kfp** (Kubeflow Pipelines SDK) ğŸ”„\n",
    "  - A toolkit for defining and orchestrating machine learning workflows and pipelines.\n",
    "  - Simplifies the creation of complex ML workflows, enabling seamless integration of multiple tasks.\n",
    "\n",
    "- **google-cloud-pipeline-components** ğŸš€\n",
    "  - Provides pre-built, reusable pipeline components designed for Google Cloud services.\n",
    "  - Speeds up development with ready-to-use components for common tasks, such as data preprocessing, training, and deployment within pipelines.\n",
    "\n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ayyRtYA026wl",
    "outputId": "d5dec5f5-27c3-4e45-da9a-2189f1b5c226"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.10/dist-packages (1.70.0)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.71.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting kfp\n",
      "  Downloading kfp-2.10.0.tar.gz (599 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m599.3/599.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting google-cloud-pipeline-components\n",
      "  Downloading google_cloud_pipeline_components-2.17.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.19.2)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.27.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (24.1)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.25.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.13.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.0.6)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.9.2)\n",
      "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.7.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.32.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (1.6.0)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (8.1.7)\n",
      "Collecting kfp-pipeline-spec==0.4.0 (from kfp)\n",
      "  Downloading kfp_pipeline_spec-0.4.0-py3-none-any.whl.metadata (301 bytes)\n",
      "Collecting kfp-server-api<2.4.0,>=2.1.0 (from kfp)\n",
      "  Downloading kfp_server_api-2.3.0.tar.gz (84 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting kubernetes<31,>=8.0.0 (from kfp)\n",
      "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 (from google-cloud-aiplatform)\n",
      "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from kfp) (6.0.2)\n",
      "Collecting requests-toolbelt<1,>=0.8.0 (from kfp)\n",
      "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.9.0)\n",
      "Collecting urllib3<2.0.0 (from kfp)\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kfp\n",
      "  Downloading kfp-2.7.0.tar.gz (441 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m441.8/441.8 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: Jinja2<4,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pipeline-components) (3.1.4)\n",
      "Collecting kfp-pipeline-spec==0.3.0 (from kfp)\n",
      "  Downloading kfp_pipeline_spec-0.3.0-py3-none-any.whl.metadata (329 bytes)\n",
      "Collecting kfp-server-api<2.1.0,>=2.0.0 (from kfp)\n",
      "  Downloading kfp-server-api-2.0.5.tar.gz (63 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting kubernetes<27,>=8.0.0 (from kfp)\n",
      "  Downloading kubernetes-26.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.65.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.64.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=3.1.2->google-cloud-pipeline-components) (3.0.2)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (2024.8.30)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp) (75.1.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.26.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp) (3.2.2)\n",
      "Downloading google_cloud_aiplatform-1.71.1-py2.py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_storage-2.18.2-py2.py3-none-any.whl (130 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_pipeline_components-2.17.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kfp_pipeline_spec-0.3.0-py3-none-any.whl (12 kB)\n",
      "Downloading kubernetes-26.1.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: kfp, kfp-server-api\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kfp: filename=kfp-2.7.0-py3-none-any.whl size=610410 sha256=52e7ec47adfe510358d295d3d3dcb631d51338ffdb0df01c4d074ddc1df0192d\n",
      "  Stored in directory: /root/.cache/pip/wheels/9e/7d/a4/f9d013e82681c9746ef10de3b00456163577a99279c5ed673d\n",
      "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-2.0.5-py3-none-any.whl size=114734 sha256=eb530694f3801b2c643f3e296e1b403fe7e78871899a6bd996250b884a2e178a\n",
      "  Stored in directory: /root/.cache/pip/wheels/ac/4f/f0/2f622aadcbf8921fb72d24f52efaffacc235f863c195c289c5\n",
      "Successfully built kfp kfp-server-api\n",
      "Installing collected packages: urllib3, protobuf, kfp-server-api, kfp-pipeline-spec, requests-toolbelt, kubernetes, google-cloud-storage, kfp, google-cloud-aiplatform, google-cloud-pipeline-components\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: requests-toolbelt\n",
      "    Found existing installation: requests-toolbelt 1.0.0\n",
      "    Uninstalling requests-toolbelt-1.0.0:\n",
      "      Successfully uninstalled requests-toolbelt-1.0.0\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.8.0\n",
      "    Uninstalling google-cloud-storage-2.8.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.8.0\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.70.0\n",
      "    Uninstalling google-cloud-aiplatform-1.70.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.70.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langsmith 0.1.137 requires requests-toolbelt<2.0.0,>=1.0.0, but you have requests-toolbelt 0.10.1 which is incompatible.\n",
      "tensorflow-metadata 1.16.1 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-cloud-aiplatform-1.71.1 google-cloud-pipeline-components-2.17.0 google-cloud-storage-2.18.2 kfp-2.7.0 kfp-pipeline-spec-0.3.0 kfp-server-api-2.0.5 kubernetes-26.1.0 protobuf-4.25.5 requests-toolbelt-0.10.1 urllib3-1.26.20\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "ac01e2e09de0464899417692a8252f01",
       "pip_warning": {
        "packages": [
         "google"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip3 install --upgrade google-cloud-aiplatform  \\\n",
    "                                 google-cloud-storage \\\n",
    "                                 kfp \\\n",
    "                                 google-cloud-pipeline-components\n",
    "# !pip install kfp==1.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Code to Restart Kernel in Google Colab\n",
    "\n",
    "This code checks if the script is running in Google Colab and, if so, restarts the Colab kernel. \n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDCoa5Vr7gUv"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "if \"google.colab\" in sys.modules:\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Code for Authenticating Google Colab with Google Cloud\n",
    "\n",
    "This code authenticates the user in Google Colab, allowing access to Google Cloud resources.\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ma_agxVZ7h_v"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "from google.colab import auth\n",
    "if \"google.colab\" in sys.modules:\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Google Cloud Configuration Details\n",
    "\n",
    "- **PROJECT_ID**: Specifies the unique Google Cloud Project ID `gen-lang-client-0561471451` where resources are created and managed.\n",
    "\n",
    "- **REGION**: The Google Cloud region `asia-south1` chosen for resource allocation and deployment, optimized for proximity to data or reduced latency.\n",
    "\n",
    "- **BUCKET_URI**: URI for the Google Cloud Storage bucket `gs://toxic_comments` where all project data and artifacts will be stored.\n",
    "\n",
    "- **BUCKET_NAME**: Extracted bucket name `toxic_comments` (from `BUCKET_URI` by removing the `gs://` prefix), as some services require only the bucket name.\n",
    "\n",
    "- **SERVICE_ACCOUNT**: Service account email `toxic-comment@gen-lang-client-0561471451.iam.gserviceaccount.com` for secure authentication and authorization with Google Cloud services.\n",
    "\n",
    "- **PIPELINE_ROOT**: Sets the root directory within the bucket (`gs://toxic_comments/pipeline_root/toxic`) for organizing all pipeline-related data, ensuring structured file management.\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AS1clRoE7uuC"
   },
   "outputs": [],
   "source": [
    "# Google Cloud project ID to identify the project where resources are created and managed.\n",
    "PROJECT_ID = \"gen-lang-client-0561471451\"\n",
    "# Specifies the Google Cloud region for resource allocation and deployment, optimizing for data location or latency.\n",
    "REGION = \"asia-south1\"\n",
    "# URI of the Google Cloud Storage bucket where data and artifacts for the project will be stored.\n",
    "BUCKET_URI = \"gs://toxic_comments\"\n",
    "# Extracts the bucket name from the full URI by removing the \"gs://\" prefix, as some services require just the name.\n",
    "BUCKET_NAME = BUCKET_URI.replace(\"gs://\", \"\")\n",
    "# Service account email for authentication and authorization, enabling secure interaction with Google Cloud resources.\n",
    "SERVICE_ACCOUNT = \"toxic-comment@gen-lang-client-0561471451.iam.gserviceaccount.com\"\n",
    "# Sets the root directory in the bucket for storing all pipeline-related data, organizing files for easy management.\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipeline_root/toxic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Setting Up the Service Account\n",
    "\n",
    "This code cell checks if the service account is configured and retrieves it if not. \n",
    "\n",
    "1. **Check Service Account**:\n",
    "   - Verifies if `SERVICE_ACCOUNT` is empty or set to the default placeholder.\n",
    "   \n",
    "2. **Retrieve Service Account**:\n",
    "   - **In Colab**: Fetches the service account using the project number.\n",
    "   - **Outside Colab**: Authenticates with Google Cloud and sets the service account from the available accounts.\n",
    "\n",
    "The final service account is displayed to confirm setup.\n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CJhi68jW7o30",
    "outputId": "28a555b7-dfbd-408a-f10b-e17f6f9a8d76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: 423620965608-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"toxic-comment@gen-lang-client-0561471451.iam.gserviceaccount.com\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    if IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"service-account-detail\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Upload Kaggle API Key\n",
    "\n",
    "To download datasets from Kaggle programmatically, follow these steps to set up the Kaggle API key.\n",
    "\n",
    "#### Steps to Get the Kaggle API Key:\n",
    "\n",
    "1. **Log in to Kaggle**: \n",
    "   - Go to [Kaggle](https://www.kaggle.com/) and log in to your account.\n",
    "\n",
    "2. **Navigate to Account Settings**:\n",
    "   - Click on your profile icon and select **Settings** and navigate to account.\n",
    "   ![My Image](images/kaggle1.png)\n",
    "3. **Generate New API Token**:\n",
    "   - Scroll down to the **API** section.\n",
    "   - Click **Create New API Token**. A file named `kaggle.json` containing your credentials will be downloaded.\n",
    "   ![My Image](images/kaggle2.png)\n",
    "\n",
    "4. **Upload `kaggle.json`**:\n",
    "   - Move the downloaded `kaggle.json` file to your project or Jupyter environment and run the following code to configure it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "K8mhcUqcIm13",
    "outputId": "85805b51-b115-4107-84f9-51b30096f749"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-babfb82b-70a9-482b-b6a4-3f078a10f955\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-babfb82b-70a9-482b-b6a4-3f078a10f955\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Kaggle Dataset Setup\n",
    "\n",
    "This code block sets up Kaggle credentials and downloads the dataset required for the Jigsaw Toxic Comment Classification Challenge.\n",
    "\n",
    "1. **Setup Kaggle Credentials**: \n",
    "   - Creates a `.kaggle` directory and moves `kaggle.json` (Kaggle API key) into it for authentication.\n",
    "   \n",
    "2. **Download Dataset**:\n",
    "   - Downloads the dataset from the competition page using Kaggle's API.\n",
    "\n",
    "3. **Extract Files**:\n",
    "   - Unzips the downloaded files (`train.csv.zip`, `test.csv.zip`, `test_labels.csv.zip`) for use in the project.\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PE4L6CmYIpIn",
    "outputId": "49c23923-5bf8-4abe-b3d7-5b4330967d5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading jigsaw-toxic-comment-classification-challenge.zip to /content\n",
      " 78% 41.0M/52.6M [00:00<00:00, 45.2MB/s]\n",
      "100% 52.6M/52.6M [00:01<00:00, 53.1MB/s]\n",
      "Archive:  jigsaw-toxic-comment-classification-challenge.zip\n",
      "  inflating: sample_submission.csv.zip  \n",
      "  inflating: test.csv.zip            \n",
      "  inflating: test_labels.csv.zip     \n",
      "  inflating: train.csv.zip           \n",
      "Archive:  test.csv.zip\n",
      "  inflating: test.csv                \n",
      "Archive:  test_labels.csv.zip\n",
      "  inflating: test_labels.csv         \n",
      "Archive:  train.csv.zip\n",
      "  inflating: train.csv               \n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n",
    "!unzip jigsaw-toxic-comment-classification-challenge\n",
    "!unzip test.csv.zip\n",
    "!unzip test_labels.csv.zip\n",
    "!unzip train.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Uploading a CSV Dataset to Google Cloud Storage (GCS)\n",
    "\n",
    "This code snippet demonstrates how to upload a CSV dataset to a specified Google Cloud Storage (GCS) bucket using Python's `pandas` and `google-cloud-storage` libraries. \n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Read the Dataset**: \n",
    "   Load the `train.csv` file into a pandas DataFrame for easy manipulation.\n",
    "   \n",
    "2. **Convert DataFrame to CSV Format**:\n",
    "   - Convert the DataFrame into a CSV-formatted string without the index column.\n",
    "   \n",
    "3. **Initialize GCS Client**:\n",
    "   - Establish a connection to GCS using `storage.Client()`.\n",
    "   - Specify the bucket name (`BUCKET_NAME`) and the desired destination path within the bucket (`dataset/train.csv`).\n",
    "   \n",
    "4. **Upload the CSV to GCS**:\n",
    "   - Upload the CSV data string directly to the specified GCS path.\n",
    "   \n",
    "5. **Confirmation**:\n",
    "   - Upon successful upload, a message is printed to confirm completion.\n",
    "\n",
    "> **Note**: Ensure that you have set up authentication with GCS and assigned a valid value to `BUCKET_NAME` before running this code.\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PV-DloUR3f6i"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "# Read the DataFrame\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Convert the DataFrame to a CSV string\n",
    "csv_data = df.to_csv(index=False)\n",
    "\n",
    "# Initialize the GCS client and specify the destination path\n",
    "blob = storage.Client().bucket(BUCKET_NAME).blob(\"dataset/train.csv\")\n",
    "\n",
    "# Upload the CSV string to GCS\n",
    "blob.upload_from_string(csv_data, content_type=\"text/csv\")\n",
    "\n",
    "print(\"Data uploaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Kubeflow Pipeline for Preprocessing Toxic Comments Dataset\n",
    "\n",
    "This code defines a machine learning pipeline using Kubeflow Pipelines SDK, designed to load, preprocess, and manage toxic comment data for training purposes. The pipeline is integrated with Google Cloud Storage (GCS) for data storage and Vertex AI for managing the pipeline job.\n",
    "\n",
    "#### Code Structure:\n",
    "\n",
    "1. **Imports**:\n",
    "   - `Kubeflow Pipelines SDK`: For defining and managing ML pipelines.\n",
    "   - `Google Cloud Storage` and `Vertex AI`: For accessing and managing AI resources on Google Cloud.\n",
    "   \n",
    "2. **Data Loading and Preprocessing Component**:\n",
    "   - This component reads raw data from GCS, preprocesses it, balances classes, and uploads the cleaned dataset back to GCS.\n",
    "\n",
    "3. **Pipeline Definition**:\n",
    "   - Defines a Kubeflow pipeline with steps to load, preprocess, and store data.\n",
    "   - This structure allows easy integration of additional components, such as model training, if required.\n",
    "\n",
    "4. **Pipeline Compilation**:\n",
    "   - The pipeline is compiled into a JSON format for execution.\n",
    "\n",
    "5. **Running the Pipeline**:\n",
    "   - Initializes Vertex AI and creates a pipeline job to execute the pipeline on Google Cloud.\n",
    "\n",
    "> **Note**: Replace placeholder variables such as `PROJECT_ID`, `BUCKET_NAME`, `PIPELINE_ROOT`, and `SERVICE_ACCOUNT` with appropriate values for your environment.\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-N4QXN_o8JDk"
   },
   "outputs": [],
   "source": [
    "# Imports for Kubeflow Pipelines and Google Cloud\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2.dsl import component, Dataset, Output\n",
    "from google.cloud import aiplatform\n",
    "import pandas as pd\n",
    "\n",
    "# Define component to load and preprocess data\n",
    "@component(\n",
    "    base_image=\"gcr.io/gen-lang-client-0561471451/toxicimage:latest\",\n",
    "    packages_to_install=[\"google-cloud-storage\", \"pandas\"]\n",
    ")\n",
    "def load_and_preprocess_data(\n",
    "    project_id: str,\n",
    "    bucket_name: str,\n",
    "    source_blob_name: str\n",
    "):\n",
    "    # Google Cloud Storage import for data access\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import io\n",
    "\n",
    "    # Initialize GCS client and access data\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    data = bucket.blob(source_blob_name).download_as_bytes()\n",
    "    df = pd.read_csv(io.BytesIO(data))\n",
    "\n",
    "    # Preprocess and balance classes\n",
    "    toxic_data = df[df[df.columns[2:]].sum(axis=1) > 0]\n",
    "    clean_data = df[df[df.columns[2:]].sum(axis=1) == 0].sample(n=16225, random_state=42)\n",
    "    balanced_data = pd.concat([toxic_data, clean_data], axis=0).sample(frac=1, random_state=42)\n",
    "\n",
    "    # Save preprocessed data back to GCS\n",
    "    bucket.blob(\"dataset/preprocess_train_data.csv\").upload_from_string(\n",
    "        balanced_data.to_csv(index=False), content_type=\"text/csv\"\n",
    "    )\n",
    "\n",
    "# Define the pipeline with parameters\n",
    "@dsl.pipeline(\n",
    "    name=\"toxic-comments-processing-pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    description=\"Pipeline to load and preprocess toxic comments dataset\"\n",
    ")\n",
    "def processing_pipeline(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    bucket_name: str = BUCKET_NAME,\n",
    "    source_blob_name: str = \"dataset/train.csv\"\n",
    "):\n",
    "    # Task to load and preprocess data\n",
    "    load_and_preprocess_data(\n",
    "        project_id=project_id,\n",
    "        bucket_name=bucket_name,\n",
    "        source_blob_name=source_blob_name\n",
    "    )\n",
    "\n",
    "# Compile pipeline to JSON\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=processing_pipeline,\n",
    "    package_path=\"processing_pipeline.json\"\n",
    ")\n",
    "\n",
    "# Initialize and run pipeline job on Vertex AI\n",
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"toxic-comments-processing\",\n",
    "    template_path=\"processing_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"bucket_name\": BUCKET_NAME,\n",
    "        \"source_blob_name\": \"dataset/train.csv\"\n",
    "    }\n",
    ")\n",
    "job.run(service_account=SERVICE_ACCOUNT, sync=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Toxic Comment Classification Pipeline\n",
    "\n",
    "This code defines a Kubeflow pipeline for toxic comment classification using BERT. The pipeline performs data downloading, tokenization, encoding, and model training, leveraging Google Cloud Storage (GCS) and Vertex AI.\n",
    "\n",
    "#### Key Components:\n",
    "\n",
    "1. **Download Data Component**:\n",
    "   - Downloads a CSV dataset from GCS and saves it locally in the pipelineâ€™s storage.\n",
    "\n",
    "2. **Tokenize and Encode Component**:\n",
    "   - Tokenizes and encodes comments using a specified BERT tokenizer.\n",
    "   - Prepares inputs for model training by generating `input_ids`, `attention_masks`, and `labels`.\n",
    "\n",
    "3. **Model Training Component**:\n",
    "   - Trains a BERT-based model using the tokenized data for multi-label classification of toxic comments.\n",
    "   - Uses AdamW optimizer and a learning rate scheduler.\n",
    "   - Saves the trained model to the pipeline's output.\n",
    "\n",
    "4. **Pipeline Definition**:\n",
    "   - Defines the sequence of steps in the `toxic_comment_classification_pipeline`.\n",
    "   - Combines all components, ensuring smooth data flow from one step to the next.\n",
    "\n",
    "5. **Pipeline Compilation and Execution**:\n",
    "   - Compiles the pipeline into a JSON format (`encode_train_pipeline.json`) for submission.\n",
    "   - Initializes and runs the pipeline job on Vertex AI.\n",
    "\n",
    "#### Notes: \n",
    ">- Ensure `PROJECT_ID`, `BUCKET_NAME`, `PIPELINE_ROOT`, and `SERVICE_ACCOUNT` are set to valid values.\n",
    ">- GPU acceleration can be requested in the training component (adjusted based on region availability).\n",
    ">- Please Access GPU before running this below cell. You can follow the steps mention in start of file \n",
    "\n",
    "This pipeline is modular and can be extended with additional components, such as data evaluation or hyperparameter tuning.\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2A0fBpxw_JSE",
    "outputId": "99dca3ac-08b0-4f7f-9014-3f55d647c0df"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/423620965608/locations/us-central1/pipelineJobs/toxic-comment-classification-pipeline-20241109072123\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/423620965608/locations/us-central1/pipelineJobs/toxic-comment-classification-pipeline-20241109072123')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/toxic-comment-classification-pipeline-20241109072123?project=423620965608\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/423620965608/locations/us-central1/pipelineJobs/toxic-comment-classification-pipeline-20241109072123 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/423620965608/locations/us-central1/pipelineJobs/toxic-comment-classification-pipeline-20241109072123 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/423620965608/locations/us-central1/pipelineJobs/toxic-comment-classification-pipeline-20241109072123 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/423620965608/locations/us-central1/pipelineJobs/toxic-comment-classification-pipeline-20241109072123 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/423620965608/locations/us-central1/pipelineJobs/toxic-comment-classification-pipeline-20241109072123 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/423620965608/locations/us-central1/pipelineJobs/toxic-comment-classification-pipeline-20241109072123 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/423620965608/locations/us-central1/pipelineJobs/toxic-comment-classification-pipeline-20241109072123 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/423620965608/locations/us-central1/pipelineJobs/toxic-comment-classification-pipeline-20241109072123\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2.dsl import component, Input, Output, Dataset, Model\n",
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "@component(\n",
    "    base_image=\"gcr.io/gen-lang-client-0561471451/toxicimage:latest\",\n",
    "    # base_image=\"python:3.8-slim\",\n",
    "    packages_to_install=[\"google-cloud-storage\", \"pandas\",\"torch\",\"transformers\"]\n",
    ")\n",
    "def download_data_from_gcs(\n",
    "    project_id: str,\n",
    "    bucket_name: str,\n",
    "    source_blob_name: str,\n",
    "    output_dataset: Output[Dataset]\n",
    "):\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import io\n",
    "\n",
    "    # Initialize storage client\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "\n",
    "    # Download data\n",
    "    data_bytes = blob.download_as_bytes()\n",
    "    data = pd.read_csv(io.BytesIO(data_bytes))\n",
    "\n",
    "    # Save data to output_dataset path\n",
    "    data.to_csv(output_dataset.path, index=False)\n",
    "    print(f\"Data downloaded and saved to {output_dataset.path}\")\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"gcr.io/gen-lang-client-0561471451/toxicimage:latest\",\n",
    "    # base_image=\"python:3.8-slim\",\n",
    "    packages_to_install=[\"google-cloud-storage\", \"pandas\",\"torch\",\"transformers\"]\n",
    ")\n",
    "def tokenize_and_encode_component(\n",
    "    input_dataset: Input[Dataset],\n",
    "    output_dataset: Output[Dataset],\n",
    "    max_length: int = 128,\n",
    "    model_name: str = 'bert-base-uncased'\n",
    "):\n",
    "    \"\"\"\n",
    "    Tokenizes and encodes the input comments using the specified tokenizer.\n",
    "    Saves the tokenized inputs, attention masks, and labels as tensors.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    from transformers import BertTokenizer\n",
    "    import os\n",
    "\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(input_dataset.path)\n",
    "    # data = data.sample(n=500, random_state=42)\n",
    "    print(\"Data loaded successfully.\")\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "\n",
    "    # Extract comments and labels\n",
    "    comments = data['comment_text'].tolist()\n",
    "    labels = data.iloc[:, 2:].values  # Assuming labels are from the third column onwards\n",
    "\n",
    "    # Initialize empty lists to store tokenized inputs and attention masks\n",
    "    input_ids_list = []\n",
    "    attention_masks_list = []\n",
    "\n",
    "    # Iterate through each comment\n",
    "    for comment in comments:\n",
    "        # Tokenize and encode the comment\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            comment,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',  # Use 'max_length' for consistent padding\n",
    "            truncation=True,       # Truncate comments longer than max_length\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Append the tokenized input and attention mask to their respective lists\n",
    "        input_ids_list.append(encoded_dict['input_ids'])\n",
    "        attention_masks_list.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Concatenate the tokenized inputs and attention masks into tensors\n",
    "    input_ids = torch.cat(input_ids_list, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks_list, dim=0)\n",
    "\n",
    "    # Convert the labels to a PyTorch tensor with the data type float32\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    # Save the tensors as a dictionary\n",
    "    tokenized_data = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_masks': attention_masks,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "    # Save the tokenized data to the output path\n",
    "    output_path = output_dataset.path\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Save the tokenized data\n",
    "    torch.save(tokenized_data, output_path)\n",
    "    print(f\"Tokenized data saved to {output_path}\")\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"gcr.io/gen-lang-client-0561471451/toxicimage:latest\",\n",
    "    packages_to_install=[\"torch\", \"transformers\"],\n",
    "    # accelerator_type='NVIDIA_TESLA_T4',  ?\\# Request a GPU type available in your region\n",
    "    # accelerator_count=1,\n",
    ")\n",
    "def train_model(\n",
    "    tokenized_data: Input[Dataset],\n",
    "    output_model: Output[Model],\n",
    "    num_labels: int = 6,\n",
    "    num_epochs: int = 5,\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 2e-5,\n",
    "    model_name: str = 'bert-base-uncased',\n",
    "):\n",
    "    \"\"\"Train a BERT model for toxic comment classification.\"\"\"\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "    import os\n",
    "\n",
    "    # Check if CUDA is available and set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load tokenized data\n",
    "    data = torch.load(tokenized_data.path, map_location=device)\n",
    "    input_ids = data['input_ids']\n",
    "    attention_masks = data['attention_masks']\n",
    "    labels = data['labels']\n",
    "\n",
    "    # Set up DataLoader\n",
    "    train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize model\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        problem_type='multi_label_classification'\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i,batch in enumerate(train_loader):\n",
    "            batch_input_ids = batch[0].to(device)\n",
    "            batch_attention_masks = batch[1].to(device)\n",
    "            batch_labels = batch[2].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=batch_input_ids,\n",
    "                attention_mask=batch_attention_masks,\n",
    "                labels=batch_labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if i%100 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Loss: {total_loss/(i+1)}\")\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    os.makedirs(output_model.path, exist_ok=True)\n",
    "    model.save_pretrained(output_model.path)\n",
    "    print(f\"Model saved to {output_model.path}\")\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"encode_train_pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def toxic_comment_classification_pipeline(\n",
    "    project_id: str,\n",
    "    bucket_name: str,\n",
    "    source_blob_name: str,\n",
    "    max_length: int = 128,\n",
    "    model_name: str = 'bert-base-uncased',\n",
    "):\n",
    "    # Download data from GCS\n",
    "    download_data_task = download_data_from_gcs(\n",
    "        project_id=project_id,\n",
    "        bucket_name=bucket_name,\n",
    "        source_blob_name=source_blob_name,\n",
    "    )\n",
    "\n",
    "    # Tokenize and encode data\n",
    "    tokenize_and_encode_task = tokenize_and_encode_component(\n",
    "        input_dataset=download_data_task.outputs['output_dataset'],\n",
    "        max_length=max_length,\n",
    "        model_name=model_name,\n",
    "    )\n",
    "\n",
    "    # Continue with other components, such as training the model\n",
    "    train_model_task = train_model(\n",
    "        tokenized_data=tokenize_and_encode_task.outputs['output_dataset']\n",
    "    )\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "# Compile the pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=toxic_comment_classification_pipeline,\n",
    "    package_path=\"encode_train_pipeline.json\"\n",
    ")\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    # location=REGION,\n",
    "    staging_bucket=BUCKET_URI,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    ")\n",
    "\n",
    "# Create and run the pipeline job\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"encode_train_pipeline\",\n",
    "    template_path=\"encode_train_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    project=PROJECT_ID,\n",
    "    # location=REGION,\n",
    "    parameter_values={\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"bucket_name\": BUCKET_NAME,\n",
    "        \"source_blob_name\": \"data/preprocess_train_data.csv\",\n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Trained model for inferencing on GCR and Integrating UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Step-by-Step Guide to Deploy the Model on Google Cloud Platform\n",
    "\n",
    "1. **Set Up the Python Environment**:\n",
    "   - Create a project directory named `toxic_comment_app`.\n",
    "\n",
    "2. **Open the Project in VS Code**:\n",
    "   - Open the `toxic_comment_app` folder in Visual Studio Code to manage and edit the files conveniently.\n",
    "\n",
    "3. **Create a Virtual Environment (Optional for Local Testing)**:\n",
    "   - Open the terminal in VS Code and run the following command to set up a Python virtual environment:\n",
    "     ```bash\n",
    "     python -m venv myenv\n",
    "     ```\n",
    "   - This step is optional and primarily for local testing to ensure all code functions are handled properly before deploying.\n",
    "\n",
    "4. **Deployment on Google Cloud**:\n",
    "   - Instead of deploying this local environment to Google Cloud, weâ€™ll create a Docker image that includes all the necessary dependencies for the app.\n",
    "   - The Docker image will handle all required libraries and versions, ensuring consistency across environments.\n",
    "\n",
    "> **Note**: The local environment setup is just for testing purposes. For deployment, the Docker image will handle all requirements, so thereâ€™s no need to push the local setup to Google Cloud.\n",
    "\n",
    "</small>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Set Up Dependencies for the App\n",
    "\n",
    "1. **Create a `requirements.txt` File**:\n",
    "   - List all necessary libraries in a `requirements.txt` file to streamline dependency management:\n",
    "     ```\n",
    "     flask\n",
    "     transformers\n",
    "     torch\n",
    "     ```\n",
    "\n",
    "2. **Install Dependencies**:\n",
    "   - Open the terminal, ensure your virtual environment is activated, and install all dependencies with:\n",
    "     ```bash\n",
    "     pip install -r requirements.txt\n",
    "     ```\n",
    "   - Mac(Activate Environment):\n",
    "      ```bash\n",
    "      source env_name/bin/activate\n",
    "      ```\n",
    "   - Windows(Activate Environment)\n",
    "      ```bash\n",
    "      env_name/scripts/activate.ps1\n",
    "      ```\n",
    "   > **Note**: Activating the Python environment is essential to ensure all dependencies are installed within this environment.\n",
    "\n",
    "3. **Library Overview**:\n",
    "   - **Transformers**: Provides tokenizers and model configurations for handling the fine-tuned BERT model saved on Google Cloud Platform (GCP).\n",
    "   - **Torch**: Used for model inference, enabling the execution of the pre-trained model.\n",
    "   - **Flask**: Manages HTTP POST requests from users and returns responses in JSON format.\n",
    "\n",
    "4. **Next Step**:\n",
    "   - We will download the fine-tuned model saved on GCP and create a directory in the local setup to store it.\n",
    "\n",
    "</small>\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<small>\n",
    "\n",
    "### Download Model Files\n",
    "\n",
    "- **Content Directory**:\n",
    "   - The directory named `content` contains the trained model files required for inference.\n",
    "   \n",
    "      <img src=\"images/frontend1.png\" alt=\"Running App\" width=\"300\"/>\n",
    " \n",
    "- **Required Files**:\n",
    "   - Download the following files from the `content` directory:\n",
    "     - `pytorch_model.bin`\n",
    "     - `config.json`\n",
    "\n",
    "- **Naming Requirement**:\n",
    "   - Ensure the files are named **`pytorch_model.bin`** and **`config.json`**. \n",
    "   - The `transformers` library relies on these specific filenames when configuring the model for inference.\n",
    "\n",
    "> **Note**: Keeping these exact filenames is crucial for compatibility with the `transformers` library.\n",
    "\n",
    "</small>\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Create the Flask App for Inference\n",
    "\n",
    "In this step, weâ€™ll write the code for the Flask application, which will handle incoming requests, run the model inference on comments, and return the results in JSON format.\n",
    "\n",
    "- **Flask Application**:\n",
    "  - The Flask app will act as an API endpoint, receiving comments from users via HTTP POST requests.\n",
    "  - It will use the downloaded model files (`pytorch_model.bin` and `config.json`) to process and infer results based on the input.\n",
    "\n",
    "- **Inference and Response**:\n",
    "  - The app will load the fine-tuned model and tokenizer using the `transformers` and `torch` libraries.\n",
    "  - When a request is received, the model will process the comment and return a JSON response containing the inference results.\n",
    "\n",
    "By setting up this Flask app, we enable an easy way to access model predictions through a simple API.\n",
    "\n",
    "> **Next Step**: Write the Flask code to handle incoming requests, run the model on the input, and respond with JSON results.\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, BertTokenizer\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "# Set the directory path containing model files\n",
    "model_dir = \"./content\"  # Make sure this directory contains 'config.json' and 'pytorch_model.bin'\n",
    "\n",
    "# Load configuration\n",
    "config = AutoConfig.from_pretrained(model_dir)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir, config=config)\n",
    "model.to(torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Flask app setup\n",
    "app = Flask(__name__)\n",
    "\n",
    "def predict_user_input(input_text, model=model, tokenizer=tokenizer):\n",
    "    # Tokenize and encode the input text\n",
    "    user_encodings = tokenizer([input_text], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    user_encodings = {key: tensor.to(model.device) for key, tensor in user_encodings.items()}  # Move tensors to device\n",
    "    \n",
    "    # Perform prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**user_encodings)\n",
    "        predictions = torch.sigmoid(outputs.logits)\n",
    "    \n",
    "    # Process predictions and convert to standard Python types\n",
    "    predicted_labels = (predictions.cpu().numpy() > 0.5).astype(int)\n",
    "    labels_list = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    result = {label: int(predicted_labels[0][i]) for i, label in enumerate(labels_list)}\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Flask route for prediction\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json()  # Use get_json for safer JSON parsing\n",
    "    if not data or 'text' not in data:\n",
    "        return jsonify({\"error\": \"Invalid input. 'text' field is required.\"}), 400\n",
    "\n",
    "    text = data['text']\n",
    "    \n",
    "    try:\n",
    "        # Predict the toxicity of the input text\n",
    "        result = predict_user_input(text)\n",
    "        return jsonify(result)\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log the exception and return an error response\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        return jsonify({\"error\": \"An error occurred during prediction.\"}), 500\n",
    "\n",
    "# Run the Flask app\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=8080)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Testing the Flask App Locally ğŸ–¥ï¸\n",
    "\n",
    "- The Flask app code is saved as `detox.py`. This app is now ready to receive POST requests from users on the `/predict` route.\n",
    "\n",
    "- **Testing the Endpoint**:\n",
    "   - To test the app locally, use the following URL as the server endpoint in the frontend code:\n",
    "     ```python\n",
    "     SERVER_URL = \"http://127.0.0.1:8080/predict\"\n",
    "     ```\n",
    "   - Set this URL as the `SERVER_URL` in your Gradio front-end code to connect to the local server.\n",
    "\n",
    "      <img src=\"images/frontend2.png\" alt=\"Running App\" width=\"700\"/>\n",
    "\n",
    "- **Next Step**: Weâ€™ll create a Docker image for deploying this app.\n",
    "\n",
    "   <img src=\"images/frontend3.png\" alt=\"Running App\" width=\"700\"/>\n",
    "\n",
    "---\n",
    "\n",
    "### Dockerfile Setup for Deployment ğŸ³\n",
    "\n",
    "Below is the code for the `Dockerfile` to containerize the Flask app, making it easy to deploy on Google Cloud Platform or other Docker-compatible services.\n",
    "\n",
    "<div style=\"background-color:; color: #000000; padding: 10px; border: 1px solid #ddd; border-radius: 5px;\">\n",
    "\n",
    "<pre><code>\n",
    "FROM python:3.8-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY detox.py .\n",
    "COPY content ./content\n",
    "\n",
    "EXPOSE 8080\n",
    "\n",
    "ENV FLASK_APP=detox.py\n",
    "ENV FLASK_RUN_HOST=0.0.0.0\n",
    "ENV PORT=8080\n",
    "\n",
    "CMD [\"flask\", \"run\", \"--host=0.0.0.0\", \"--port=8080\"]\n",
    "\n",
    "</code></pre>\n",
    "</div>\n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Docker Commands for Building the Image ğŸ³\n",
    "\n",
    "**Step 1**: Check Docker Version  \n",
    "\n",
    "   <img src=\"images/frontend4.png\" alt=\"Running App\" width=\"700\"/> \n",
    "   \n",
    "   - **Command**: `docker --version`  \n",
    "   - This command verifies that Docker is installed and provides the current version.\n",
    "\n",
    "\n",
    "**Step 2**: Build the Docker Image ğŸ“¦  \n",
    "\n",
    "   <img src=\"images/frontend5.png\" alt=\"Running App\" width=\"800\"/>\n",
    "\n",
    "   - **Command**: \n",
    "     ```bash\n",
    "     docker build -t gcr.io/gen-lang-client-0561471451/toxic-comment-classifier .\n",
    "     ```\n",
    "   - This command builds the Docker image and tags it with the specified name (`toxic-comment-classifier`) in Google Container Registry.\n",
    "\n",
    "   > âœ… After running this, your Docker image will be built successfully!\n",
    "</small>\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Setting Up Google Cloud SDK to Push Docker Image to GCP ğŸš€\n",
    "\n",
    "To push the Docker image to Google Cloud, we first need to install the **gcloud SDK CLI** and configure it to connect the local environment with Google Cloud Platform (GCP).\n",
    "\n",
    "1. **Download the gcloud SDK CLI**:\n",
    "   - Use the following link to download and install the gcloud SDK CLI:\n",
    "     [Google Cloud SDK Installation Guide](https://cloud.google.com/sdk/docs/install) ğŸŒ\n",
    "\n",
    "      <!-- <img src=\"images/frontend6.png\" alt=\"Running App\" width=\"400\"/> -->\n",
    "\n",
    "2. **Set Up and Verify Installation**:\n",
    "   - After installation, set up gcloud and verify the version to confirm successful installation.\n",
    "   \n",
    "      <img src=\"images/frontend6.png\" alt=\"Running App\" width=\"400\"/>\n",
    "\n",
    "3. **Configuration**:\n",
    "   - Once the SDK is installed, youâ€™ll configure it to authenticate and connect with your GCP account.\n",
    "   - This setup is essential for deploying the model on GCP.\n",
    "\n",
    "> **Next Step**: With the SDK set up, youâ€™ll be ready to push your Docker image to Google Cloud and proceed with the deployment.\n",
    "</small>\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Configuring and Pushing Docker Image to Google Cloud Platform ğŸš€\n",
    "\n",
    "1. **Initialize gcloud Configuration**  \n",
    "   - **Command**: `gcloud init`  \n",
    "   - This command initiates gcloud configuration and will prompt you to authenticate your Google account and select the project ID to link with Google Cloud.\n",
    "\n",
    "      <img src=\"images/frontend7.png\" alt=\"Running App\" width=\"700\"/>\n",
    "\n",
    "   - **Note**: The process includes verification steps to link your Google account, so some sensitive steps are not shown here.\n",
    "\n",
    "2. **Configure Docker with gcloud**  \n",
    "   - **Command**: `gcloud auth configure-docker`  \n",
    "   - This command authorizes Docker to use your Google Cloud credentials, enabling seamless interaction between Docker and Google Cloud.\n",
    "\n",
    "3. **Push Docker Image to Google Cloud Registry**  \n",
    "   - **Command**: \n",
    "     ```bash\n",
    "     docker push gcr.io/[project-id]/toxic-comment-classifier\n",
    "     ```\n",
    "\n",
    "     <img src=\"images/frontend8.png\" alt=\"Running App\" width=\"700\"/>\n",
    "\n",
    "   - This command pushes the Docker image to your Google Cloud Container Registry, where it will be stored securely in your Google Cloud account.\n",
    "\n",
    "> ğŸ“ **Result**: Your Docker image is now saved in Google Cloud Registry, ready for deployment on Google Cloud Platform!\n",
    "</small>\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Deploying the Model on Google Cloud Run ğŸ‰\n",
    "\n",
    "The final step is to deploy our Dockerized model on **Google Cloud Run**! This will provide a live URL to access the model API, ready to handle requests.\n",
    "\n",
    "1. **Deploy the Docker Image to Cloud Run**  \n",
    "   - **Command**: \n",
    "     ```bash\n",
    "     gcloud run deploy toxic-comment-api --image gcr.io/[project id]/toxic-comment-classifier --platform managed --region asia-south1 --allow-unauthenticated --memory 1Gi\n",
    "     ```\n",
    "   - **Explanation**:\n",
    "     - Allocates **1Gi** memory to accommodate the model size (approximately 551Mi).\n",
    "     - **`--allow-unauthenticated`**: Makes the API publicly accessible.\n",
    "     - After running this command, youâ€™ll receive a URL that serves as the endpoint for the API.\n",
    "\n",
    "   - **Your API URL**:\n",
    "     - Example URL: [https://toxic-comment-api-423620965608.asia-south1.run.app](https://toxic-comment-api-423620965608.asia-south1.run.app)\n",
    "     - This URL will receive POST requests from the frontend and respond with predictions. ğŸš€\n",
    "\n",
    "\n",
    "2. **Stopping the Service (Optional)**  \n",
    "   - **Command**: \n",
    "     ```bash\n",
    "     gcloud run services delete toxic-comment-api --region asia-south1\n",
    "     ```\n",
    "   - This command stops and deletes the deployment if you ever need to take it down.\n",
    "\n",
    "ğŸ‰ **Congratulations!** Your model is now live on Google Cloud Platform and ready to handle requests. Your deployment journey is complete â€” hurray! ğŸŠ\n",
    "</small>\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Set Up Project Directory\n",
    "\n",
    "1. Create a project directory named `toxic_comment_app`.\n",
    "2. Open this directory in **VS Code**.\n",
    "3. Set up a Python virtual environment within this directory.\n",
    "   \n",
    "   <!-- ![Setup Directory](attachment:image.png) -->\n",
    "</small>\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Install Dependencies\n",
    "\n",
    "- Activate the virtual environment and install dependencies.\n",
    "\n",
    "   <img src=\"images/frontend9.png\" alt=\"Running App\" width=\"700\"/>\n",
    "\n",
    "- Install this below required dependencies\n",
    "\n",
    "   <img src=\"images/frontend10.png\" alt=\"Running App\" width=\"400\"/>\n",
    "\n",
    "   Run this code\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "> **Note**: Ensure the virtual environment is activated for installing libraries within the app's environment.\n",
    "\n",
    "### Set Up App Files\n",
    "\n",
    "1. Create the main application file named `app.py`.\n",
    "2. Create a `.env` file to securely store API keys and other sensitive information.\n",
    "   \n",
    "   <img src=\"images/frontend13.png\" alt=\"Running App\" width=\"400\"/>\n",
    "\n",
    "----------\n",
    "\n",
    "<img src=\"images/frontend12.png\" alt=\"Running App\" width=\"250\"/>\n",
    "\n",
    "#### The final directory structure should look like the image above.\n",
    "\n",
    "</small>\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Features of the Toxic Comment App\n",
    "\n",
    "The app includes the following features:\n",
    "1. Accepts user comments as input.\n",
    "2. Sends the input to a cloud-deployed model via POST request at the URL:\n",
    "   - `https://toxic-comment-model.herokuapp.com/predict`\n",
    "3. Displays the model's prediction on the UI.\n",
    "4. If the comment is toxic, it will be processed using the Gemini Model API (credentials in `.env`) to generate a stabilized version of the comment.\n",
    "\n",
    "The **Gemini API** can be created at [Google Gemini AI Studio](https://ai.google.dev/aistudio).\n",
    "</small>\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toxic Comment App Code (save this as `app.py`)\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Google Generative AI model with Gemini\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0.9,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "# Define the server URL of your back-end Flask app\n",
    "SERVER_URL = \"https://toxic-comment-api-423620965608.asia-south1.run.app/predict\"  # Update this URL if needed\n",
    "\n",
    "# Define the classes based on the back-end setup\n",
    "classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "# Define the inference function to make a POST request to the Flask back-end\n",
    "def predict_user_input(input_text):\n",
    "    try:\n",
    "        # Send a POST request to the back-end API with the input text\n",
    "        response = requests.post(SERVER_URL, json={\"text\": input_text})\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        \n",
    "        # Parse response JSON\n",
    "        result = response.json()\n",
    "        predicted_labels = [label for label, value in result.items() if value == 1]\n",
    "        \n",
    "        # Determine if any toxic category is present\n",
    "        is_toxic = bool(predicted_labels)\n",
    "\n",
    "        # Return the result as a string and the is_toxic flag\n",
    "        return \", \".join(predicted_labels) if predicted_labels else \"No toxic categories detected.\", is_toxic\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Error: {e}\", False\n",
    "\n",
    "# Define function to use `ChatGoogleGenerativeAI` for stabilization\n",
    "def stabilize_comment(input_text):\n",
    "    # Request to Gemini for stabilization with a specific prompt\n",
    "    prompt = f\"I have given you a text, Assume that you are amazing detoxifier of comments and only give me a single detoxified comment as a output.\\neg: you are an idiot person---> stabilized comment ---> You're making progress, and with some practice, you'll get even better at this.\\n\\n'{input_text}'\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content if response else \"Error: No response from the API\"\n",
    "\n",
    "# Gradio interface function\n",
    "def gradio_interface(comment):\n",
    "    # Get toxicity classification and check if toxic\n",
    "    toxicity_result, is_toxic = predict_user_input(comment)\n",
    "\n",
    "    # Stabilize the comment if toxic\n",
    "    stabilized_comment = \"Comment is not toxic. No stabilization needed.\"\n",
    "    if is_toxic:\n",
    "        stabilized_comment = stabilize_comment(comment)\n",
    "\n",
    "    return toxicity_result, stabilized_comment\n",
    "\n",
    "# Gradio UI setup\n",
    "interface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=gr.Textbox(label=\"Enter a Comment\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Toxicity Classification\"),\n",
    "        gr.Textbox(label=\"Stabilized Comment\")\n",
    "    ],\n",
    "    title=\"Toxic Comment Classifier with Stabilization\",\n",
    "    description=\"Classify a comment into toxicity categories and stabilize it if toxic.\",\n",
    "    examples=[[\"You are such an idiot!\"], [\"I hope you have a great day!\"], [\"That was an awful thing to say!\"]]\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "interface.launch(share = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### Running the App\n",
    "\n",
    "- Run the app using Gradio, which will create both a local and public URL for testing.\n",
    "   \n",
    "   <img src=\"images/frontend14.png\" alt=\"Running App\" width=\"500\"/>\n",
    "   \n",
    "- Use the provided link to test the appâ€™s live functionality.\n",
    "\n",
    "   <img src=\"images/frontend15.png\" alt=\"Running App\" width=\"700\"/>\n",
    "\n",
    "</small>\n",
    "\n",
    "\n",
    "---------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<medium>\n",
    "\n",
    "ğŸŠ **Congratulations!** ğŸŠ Youâ€™ve successfully built and deployed a full-stack Toxic Comment App! This app can now:\n",
    "1. Accept comments from users.\n",
    "2. Classify toxicity levels.\n",
    "3. Stabilize comments when needed for a positive interaction.\n",
    "\n",
    "> **High-five** for reaching the finish line! ğŸ‘ Now share your live URL and celebrate your hard work! ğŸŒŸ\n",
    "\n",
    "</medium>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
